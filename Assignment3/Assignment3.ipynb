{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Assignment3/Grocery_Items_24.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaasa\\AppData\\Local\\Temp\\ipykernel_2400\\2478680847.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_encoded = df.applymap(lambda x: 1 if x else 0)\n",
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       antecedents                      consequents  antecedent support  \\\n",
      "0              (1)                              (0)                 1.0   \n",
      "1              (0)                              (1)                 1.0   \n",
      "2              (2)                              (0)                 1.0   \n",
      "3              (0)                              (2)                 1.0   \n",
      "4              (0)                              (3)                 1.0   \n",
      "...            ...                              ...                 ...   \n",
      "173047         (9)  (2, 1, 6, 8, 3, 7, 10, 0, 4, 5)                 1.0   \n",
      "173048        (10)   (2, 1, 6, 8, 3, 7, 9, 0, 4, 5)                 1.0   \n",
      "173049         (0)  (2, 1, 6, 8, 3, 7, 9, 10, 4, 5)                 1.0   \n",
      "173050         (4)  (2, 1, 6, 8, 3, 7, 9, 10, 0, 5)                 1.0   \n",
      "173051         (5)  (2, 1, 6, 8, 3, 7, 9, 10, 0, 4)                 1.0   \n",
      "\n",
      "        consequent support  support  confidence  lift  leverage  conviction  \\\n",
      "0                      1.0      1.0         1.0   1.0       0.0         inf   \n",
      "1                      1.0      1.0         1.0   1.0       0.0         inf   \n",
      "2                      1.0      1.0         1.0   1.0       0.0         inf   \n",
      "3                      1.0      1.0         1.0   1.0       0.0         inf   \n",
      "4                      1.0      1.0         1.0   1.0       0.0         inf   \n",
      "...                    ...      ...         ...   ...       ...         ...   \n",
      "173047                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "173048                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "173049                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "173050                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "173051                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "\n",
      "        zhangs_metric  \n",
      "0                 0.0  \n",
      "1                 0.0  \n",
      "2                 0.0  \n",
      "3                 0.0  \n",
      "4                 0.0  \n",
      "...               ...  \n",
      "173047            0.0  \n",
      "173048            0.0  \n",
      "173049            0.0  \n",
      "173050            0.0  \n",
      "173051            0.0  \n",
      "\n",
      "[173052 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Assuming you have a DataFrame called 'df' with the transaction data\n",
    "\n",
    "# Encode the data as binary values (0 or 1)\n",
    "df_encoded = df.applymap(lambda x: 1 if x else 0)\n",
    "\n",
    "# Perform Apriori algorithm with a minimum support of 0.01\n",
    "frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)\n",
    "\n",
    "# Generate association rules with a minimum confidence of 0.1\n",
    "association_rules_df = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.1)\n",
    "\n",
    "# Print the association rules\n",
    "print(association_rules_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaasa\\AppData\\Local\\Temp\\ipykernel_2400\\2629326324.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_encoded = df.applymap(lambda x: 1 if x else 0)\n",
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 12 elements, new values have 3 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kaasa\\Documents\\GitHub\\Data-Mining-1\\Assignment3\\Assignment3.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/GitHub/Data-Mining-1/Assignment3/Assignment3.ipynb#X21sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Create a DataFrame from the count results\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/GitHub/Data-Mining-1/Assignment3/Assignment3.ipynb#X21sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m count_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(count_results, index\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m])  \u001b[39m# Set an arbitrary index, in this case, [1]\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/GitHub/Data-Mining-1/Assignment3/Assignment3.ipynb#X21sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m count_df\u001b[39m.\u001b[39;49mcolumns \u001b[39m=\u001b[39m mct_values\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/GitHub/Data-Mining-1/Assignment3/Assignment3.ipynb#X21sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Create a heatmap using Seaborn\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kaasa/Documents/GitHub/Data-Mining-1/Assignment3/Assignment3.ipynb#X21sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m, \u001b[39m6\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:6218\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   6216\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   6217\u001b[0m     \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(\u001b[39mself\u001b[39m, name)\n\u001b[1;32m-> 6218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__setattr__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name, value)\n\u001b[0;32m   6219\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m   6220\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32mproperties.pyx:69\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:767\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    763\u001b[0m \u001b[39mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[0;32m    764\u001b[0m \u001b[39mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[0;32m    765\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    766\u001b[0m labels \u001b[39m=\u001b[39m ensure_index(labels)\n\u001b[1;32m--> 767\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mset_axis(axis, labels)\n\u001b[0;32m    768\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32mc:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:227\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_axis\u001b[39m(\u001b[39mself\u001b[39m, axis: AxisInt, new_labels: Index) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    226\u001b[0m     \u001b[39m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_set_axis(axis, new_labels)\n\u001b[0;32m    228\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis] \u001b[39m=\u001b[39m new_labels\n",
      "File \u001b[1;32mc:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\base.py:85\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[39melif\u001b[39;00m new_len \u001b[39m!=\u001b[39m old_len:\n\u001b[1;32m---> 85\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     86\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLength mismatch: Expected axis has \u001b[39m\u001b[39m{\u001b[39;00mold_len\u001b[39m}\u001b[39;00m\u001b[39m elements, new \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     87\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalues have \u001b[39m\u001b[39m{\u001b[39;00mnew_len\u001b[39m}\u001b[39;00m\u001b[39m elements\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 12 elements, new values have 3 elements"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset into a Pandas DataFrame (assuming it's named 'df')\n",
    "\n",
    "# Encode the data as binary values (0 or 1)\n",
    "df_encoded = df.applymap(lambda x: 1 if x else 0)\n",
    "\n",
    "# Specify the msv and mct values\n",
    "msv_values = [0.001, 0.005, 0.01, 0.05]\n",
    "mct_values = [0.05, 0.075, 0.1]\n",
    "\n",
    "# Create an empty dictionary to store the count results\n",
    "count_results = {}\n",
    "\n",
    "# Iterate over the msv and mct values\n",
    "for msv in msv_values:\n",
    "    for mct in mct_values:\n",
    "        # Perform Apriori algorithm with the current msv and mct\n",
    "        frequent_itemsets = apriori(df_encoded, min_support=msv, use_colnames=True)\n",
    "        association_rules_df = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=mct)\n",
    "        \n",
    "        # Count the number of association rules\n",
    "        count_results[(msv, mct)] = len(association_rules_df)\n",
    "\n",
    "# Create a DataFrame from the count results\n",
    "count_df = pd.DataFrame(count_results, index=[1])  # Set an arbitrary index, in this case, [1]\n",
    "count_df.columns = mct_values\n",
    "\n",
    "# Create a heatmap using Seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(count_df, annot=True, cmap=\"YlGnBu\", fmt=\"g\", cbar=True)\n",
    "plt.xlabel(\"Minimum Confidence Threshold (mct)\")\n",
    "plt.ylabel(\"Minimum Support Value (msv)\")\n",
    "plt.title(\"Association Rule Counts Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 2</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_loader(path):\n",
    "    img =  cv2.imread(path)\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    img = img.reshape(3,331,331)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping labels to 0 to 3 for all the four classes\n",
    "labmap = {0: \"n02089078-black-and-tan_coonhound\"\n",
    "          ,1: \"n02091831-Saluki\"\n",
    "          ,2:\"n02092002-Scottish_deerhound\"\n",
    "          ,3:\"n02095314-wire-haired_fox_terrier\"} \n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "data_set = dset.ImageFolder(root=\"../DataSet/ProcessedDatasets/\",loader = image_loader) # should we add any transforms or split in\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing the data loader\n",
    "Train_Loader =  DataLoader(dataset=data_set,shuffle=True,batch_size=BATCH_SIZE)\n",
    "\n",
    "for img,label in Train_Loader:\n",
    "    print(img[0].shape)\n",
    "    plt.imshow(img[0].reshape(331,331,3))\n",
    "    plt.title(labmap[int(label[0])])\n",
    "    print(type(img[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(CNN, self).__init__()\n",
    "        self.Layer1 = nn.Conv2d(3,8,kernel_size=(3,3),bias=True)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.hidden = nn.Linear(8*164*164,16)\n",
    "        self.fc = nn.Linear(16,4)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.Layer1(x.float())\n",
    "        x = self.relu(x) # should be after max_pool or before?\n",
    "        x = self.max_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return nn.functional.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN5(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(CNN5, self).__init__()\n",
    "        self.Layer1 = nn.Conv2d(3,8,kernel_size=(5,5),bias=True)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.hidden = nn.Linear(8*163*163,16) \n",
    "        self.fc = nn.Linear(16,4)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.Layer1(x.float())\n",
    "        x = self.relu(x) # should be after max_pool or before?\n",
    "        x = self.max_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return nn.functional.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN7(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(CNN7, self).__init__()\n",
    "        self.Layer1 = nn.Conv2d(3,8,kernel_size=(7,7),bias=True) \n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.hidden = nn.Linear(8*162*162,16)\n",
    "        self.fc = nn.Linear(16,4)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.Layer1(x.float())\n",
    "        x = self.relu(x) # should be after max_pool or before?\n",
    "        x = self.max_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return nn.functional.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,val_data = random_split(data_set, [0.8, 0.2])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda' \n",
    "model = CNN().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_epochs = 10\n",
    "training_losses = []\n",
    "training_accuracies = []\n",
    "validation_losses = []\n",
    "validation_accuracies = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #optimizer.zero_grad()  # Clear gradients\n",
    "        outputs = model(images)\n",
    "        loss = F.cross_entropy( outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    training_losses.append(loss.item())\n",
    "    training_accuracies.append(100 * train_correct / train_total)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    validation_losses.append(val_loss / len(val_loader))\n",
    "    validation_accuracies.append(100 * val_correct / val_total)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {loss.item():.4f}, Training Accuracy: {100 * train_correct / train_total:.2f}%, Validation Loss: {val_loss / len(val_loader):.4f}, Validation Accuracy: {100 * val_correct / val_total:.2f}%')\n",
    "plt.plot(training_accuracies, label='Training accuracy')\n",
    "plt.plot(validation_accuracies, label='Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,val_data = random_split(data_set, [0.8, 0.2])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda' \n",
    "model = CNN5().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_epochs = 10\n",
    "training_losses = []\n",
    "training_accuracies = []\n",
    "validation_losses = []\n",
    "validation_accuracies = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #optimizer.zero_grad()  # Clear gradients\n",
    "        outputs = model(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    training_losses.append(loss.item())\n",
    "    training_accuracies.append(100 * train_correct / train_total)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    validation_losses.append(val_loss / len(val_loader))\n",
    "    validation_accuracies.append(100 * val_correct / val_total)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {loss.item():.4f}, Training Accuracy: {100 * train_correct / train_total:.2f}%, Validation Loss: {val_loss / len(val_loader):.4f}, Validation Accuracy: {100 * val_correct / val_total:.2f}%')\n",
    "plt.plot(training_accuracies, label='Training accuracy')\n",
    "plt.plot(validation_accuracies, label='Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,val_data = random_split(data_set, [0.8, 0.2])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda' \n",
    "model = CNN7().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_epochs = 10\n",
    "training_losses = []\n",
    "training_accuracies = []\n",
    "validation_losses = []\n",
    "validation_accuracies = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #optimizer.zero_grad()  # Clear gradients\n",
    "        outputs = model(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    training_losses.append(loss.item())\n",
    "    training_accuracies.append(100 * train_correct / train_total)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    validation_losses.append(val_loss / len(val_loader))\n",
    "    validation_accuracies.append(100 * val_correct / val_total)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {loss.item():.4f}, Training Accuracy: {100 * train_correct / train_total:.2f}%, Validation Loss: {val_loss / len(val_loader):.4f}, Validation Accuracy: {100 * val_correct / val_total:.2f}%')\n",
    "\n",
    "\n",
    "plt.plot(training_accuracies, label='Training accuracy')\n",
    "plt.plot(validation_accuracies, label='Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
