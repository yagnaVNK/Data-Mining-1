{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import transforms\n",
    "from torchsummary import  summary\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# referred from https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\n",
    "\n",
    "labmap = {0: \"n02089078-black-and-tan_coonhound\",\n",
    "          1: \"n02091831-Saluki\",\n",
    "          2: \"n02092002-Scottish_deerhound\",\n",
    "          3: \"n02095314-wire-haired_fox_terrier\"}\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.images = self.load_images()\n",
    "\n",
    "    def load_images(self):\n",
    "        images = []\n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(self.root_dir, class_name)\n",
    "            for filename in os.listdir(class_path):\n",
    "                image_path = os.path.join(class_path, filename)\n",
    "                images.append((image_path, self.class_to_idx[class_name]))\n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "root_folder = '../DataSet/ProcessedDatasets/'\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5),(0.5))\n",
    "                                ])\n",
    "\n",
    "dog_dataset = CustomDataset(root_folder, transform=transform)\n",
    "\n",
    "batch_size = 16\n",
    "data_loader = DataLoader(dog_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load the pre-trained ResNet-18 model\n",
    "resnet_model = resnet18(pretrained=True)\n",
    "# Remove the final fully connected layer\n",
    "resnet_model = torch.nn.Sequential(*(list(resnet_model.children())[:-2]))\n",
    "resnet_model = resnet_model.to('cuda')\n",
    "# Set the model to evaluation mode\n",
    "resnet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#referred from https://pytorch.org/vision/stable/feature_extraction.html\n",
    "#referred from https://kozodoi.me/blog/20210527/extracting-features\n",
    "\n",
    "batch_size = 32\n",
    "data_loader = DataLoader(dog_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Extract features using the pre-trained ResNet-18 model\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        features = resnet_model(images)        \n",
    "        all_features.append(features.cpu().squeeze().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "# Concatenate features and labels\n",
    "features_reshaped = np.concatenate(all_features, axis=0).reshape(-1, 512 * 7 * 7)\n",
    "all_labels = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#referred from https://scikit-learn.org/0.15/modules/generated/sklearn.decomposition.PCA.html\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents_dog = pca.fit_transform(features_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:392: UserWarning: Exited at iteration 217 with accuracies \n",
      "[3.27773152e-13 3.00833888e-05 2.12821777e-05 1.23635816e-05\n",
      " 2.02417273e-05]\n",
      "not reaching the requested tolerance 2.3653836898059478e-05.\n",
      "  _, diffusion_map = lobpcg(\n",
      "c:\\Users\\kaasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1152: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#referred from https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans, SpectralClustering, BisectingKMeans\n",
    "\n",
    "\n",
    "# (a) K-means clustering\n",
    "kmeans = KMeans(n_clusters=4, init='random')\n",
    "kmeans_labels = kmeans.fit_predict(principalComponents_dog)\n",
    "\n",
    "# (b) KMeans with init='k-means++'\n",
    "kmeans_pp = KMeans(n_clusters=4, init='k-means++')\n",
    "kmeans_pp_labels = kmeans_pp.fit_predict(principalComponents_dog)\n",
    "\n",
    "# (c) Bisecting K-means\n",
    "bisecting_kmeans = BisectingKMeans(n_clusters=4, init='random')\n",
    "bisecting_kmeans_labels = bisecting_kmeans.fit_predict(principalComponents_dog)\n",
    "\n",
    "# (d) Spectral clustering\n",
    "spectral_clustering = SpectralClustering(n_clusters=4)\n",
    "spectral_labels = spectral_clustering.fit_predict(principalComponents_dog)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The no of clusters for eps 0.99 and min_samples 3 is 4\n"
     ]
    }
   ],
   "source": [
    "#refered from https://scikit-learn.org/stable/modules/clustering.html#dbscan\n",
    "#referred from https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "#referred from https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fowlkes_mallows_score.html\n",
    "#referred from https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html\n",
    "\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import fowlkes_mallows_score, silhouette_score\n",
    "\n",
    "# (e) DBSCAN\n",
    "dbscan = DBSCAN(eps=0.99, min_samples=3)\n",
    "dbscan_labels = dbscan.fit_predict(principalComponents_dog)\n",
    "print(f\"The no of clusters for eps 0.99 and min_samples 3 is {len(set(dbscan_labels))}\")\n",
    "#  Agglomerative clustering\n",
    "# (f) Single link (MIN)\n",
    "single_link = AgglomerativeClustering(n_clusters=4, linkage='single')\n",
    "single_link_labels = single_link.fit_predict(principalComponents_dog)\n",
    "\n",
    "# (g) Complete link (MAX)\n",
    "complete_link = AgglomerativeClustering(n_clusters=4, linkage='complete')\n",
    "complete_link_labels = complete_link.fit_predict(principalComponents_dog)\n",
    "\n",
    "# (h) Group Average\n",
    "group_average = AgglomerativeClustering(n_clusters=4, linkage='average')\n",
    "group_average_labels = group_average.fit_predict(principalComponents_dog)\n",
    "\n",
    "# (i) Ward's method\n",
    "ward = AgglomerativeClustering(n_clusters=4, linkage='ward')\n",
    "ward_labels = ward.fit_predict(principalComponents_dog)\n",
    "\n",
    "# Clustering evaluation metrics\n",
    "def evaluate_clustering(labels, true_labels):\n",
    "    fowlkes_mallows = fowlkes_mallows_score(true_labels, labels)\n",
    "    silhouette = silhouette_score(principalComponents_dog, labels)\n",
    "    return fowlkes_mallows, silhouette\n",
    "\n",
    "# Ground truth labels (assuming you have them)\n",
    "true_labels = all_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> To get 4 clusters for dbscan I used eps = 0.99 and min_samples = 3</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN Scores (Fowlkes-Mallows, Silhouette): (0.5005859868587448, -0.2579051)\n",
      "Single Link Scores (Fowlkes-Mallows, Silhouette): (0.5037631189299173, 0.034081228)\n",
      "Complete Link Scores (Fowlkes-Mallows, Silhouette): (0.5657348052505528, 0.42517817)\n",
      "Group Average Scores (Fowlkes-Mallows, Silhouette): (0.5948711505892431, 0.45314533)\n",
      "Ward Scores (Fowlkes-Mallows, Silhouette): (0.5952182342715089, 0.44958904)\n",
      "\n",
      "Rankings based on Fowlkes-Mallows Index:\n",
      "1. Ward's Method\n",
      "2. Group Average\n",
      "3. Complete Link\n",
      "4. Single Link\n",
      "5. DBSCAN\n",
      "\n",
      "Rankings based on Silhouette Coefficient:\n",
      "1. Group Average\n",
      "2. Ward's Method\n",
      "3. Complete Link\n",
      "4. Single Link\n",
      "5. DBSCAN\n"
     ]
    }
   ],
   "source": [
    "dbscan_scores = evaluate_clustering(dbscan_labels, true_labels)\n",
    "# Evaluate Agglomerative clustering methods\n",
    "single_link_scores = evaluate_clustering(single_link_labels, true_labels)\n",
    "complete_link_scores = evaluate_clustering(complete_link_labels, true_labels)\n",
    "group_average_scores = evaluate_clustering(group_average_labels, true_labels)\n",
    "ward_scores = evaluate_clustering(ward_labels, true_labels)\n",
    "\n",
    "# Print the evaluation scores\n",
    "print(\"DBSCAN Scores (Fowlkes-Mallows, Silhouette):\", dbscan_scores)\n",
    "print(\"Single Link Scores (Fowlkes-Mallows, Silhouette):\", single_link_scores)\n",
    "print(\"Complete Link Scores (Fowlkes-Mallows, Silhouette):\", complete_link_scores)\n",
    "print(\"Group Average Scores (Fowlkes-Mallows, Silhouette):\", group_average_scores)\n",
    "print(\"Ward Scores (Fowlkes-Mallows, Silhouette):\", ward_scores)\n",
    "\n",
    "# Rank methods based on Fowlkes-Mallows index\n",
    "methods_fm_rank = sorted([(dbscan_scores[0], 'DBSCAN'),\n",
    "                          (single_link_scores[0], 'Single Link'),\n",
    "                          (complete_link_scores[0], 'Complete Link'),\n",
    "                          (group_average_scores[0], 'Group Average'),\n",
    "                          (ward_scores[0], \"Ward's Method\")], reverse=True)\n",
    "\n",
    "# Rank methods based on Silhouette Coefficient\n",
    "methods_silhouette_rank = sorted([(dbscan_scores[1], 'DBSCAN'),\n",
    "                                 (single_link_scores[1], 'Single Link'),\n",
    "                                 (complete_link_scores[1], 'Complete Link'),\n",
    "                                 (group_average_scores[1], 'Group Average'),\n",
    "                                 (ward_scores[1], \"Ward's Method\")], reverse=True)\n",
    "\n",
    "# Print the rankings\n",
    "print(\"\\nRankings based on Fowlkes-Mallows Index:\")\n",
    "for rank, method in enumerate(methods_fm_rank, 1):\n",
    "    print(f\"{rank}. {method[1]}\")\n",
    "\n",
    "print(\"\\nRankings based on Silhouette Coefficient:\")\n",
    "for rank, method in enumerate(methods_silhouette_rank, 1):\n",
    "    print(f\"{rank}. {method[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>References</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\n",
    "2. https://pytorch.org/vision/stable/feature_extraction.html\n",
    "3. https://kozodoi.me/blog/20210527/extracting-features\n",
    "4. https://scikit-learn.org/0.15/modules/generated/sklearn.decomposition.PCA.html\n",
    "5. https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods\n",
    "6. https://scikit-learn.org/stable/modules/clustering.html#dbscan\n",
    "7. https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "8. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fowlkes_mallows_score.html\n",
    "9. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
